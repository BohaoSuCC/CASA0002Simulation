{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dr Dennett's Guide to Spatial Interaction Modelling -  Part 2b: Constrained Models - doubly constrained\n",
    "\n",
    "Code translated to Python by Philip Wilkinson\n",
    "\n",
    "## Recap\n",
    "\n",
    "Last workshop we built on our knowledge of unconstrained models by introducing the two singly constrained models. Within these models, more information can be added in, if it is available, in order to improve the model fit. We also covered some potential use cases. We could see that by adding constraints in the model, using information that we already had, could improve how well the model is able to model flows between boroughs, but also we could choose the model depending on what behaviour or values we would like to change to see how it would in theory affect flows.\n",
    "\n",
    "This week we will build on that by introducing the doubly constrained model which contains as much information as possible into the model. Before that however, we need to bring in the data from the first practical so that we can use it to cmopare the results of the new practical:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the necessary libraries \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import seaborn as sns\n",
    "import folium\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import statsmodels.formula.api as smf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up the metric calculations\n",
    "def CalcRSqaured(observed, estimated):\n",
    "    \"\"\"Calculate the r^2 from a series of observed and estimated target values\n",
    "    inputs:\n",
    "    Observed: Series of actual observed values\n",
    "    estimated: Series of predicted values\"\"\"\n",
    "    \n",
    "    r, p = scipy.stats.pearsonr(observed, estimated)\n",
    "    R2 = r **2\n",
    "    \n",
    "    return R2\n",
    "\n",
    "def CalcRMSE(observed, estimated):\n",
    "    \"\"\"Calculate Root Mean Square Error between a series of observed and estimated values\n",
    "    inputs:\n",
    "    Observed: Series of actual observed values\n",
    "    estimated: Series of predicted values\"\"\"\n",
    "    \n",
    "    res = (observed -estimated)**2\n",
    "    RMSE = round(sqrt(res.mean()), 3)\n",
    "    \n",
    "    return RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#read in the cdatasub from the first week\n",
    "cdatasub = pd.read_csv(\"Data/cdatasub1.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrigCodeNew</th>\n",
       "      <th>DestCodeNew</th>\n",
       "      <th>TotalNoIntra</th>\n",
       "      <th>Orig</th>\n",
       "      <th>OrigCode</th>\n",
       "      <th>Dest</th>\n",
       "      <th>DestCode</th>\n",
       "      <th>Total</th>\n",
       "      <th>WorksFromHome</th>\n",
       "      <th>Underground</th>\n",
       "      <th>...</th>\n",
       "      <th>Dj1_destpop</th>\n",
       "      <th>Dj2_destsal</th>\n",
       "      <th>offset</th>\n",
       "      <th>Dist</th>\n",
       "      <th>unconstrainedEst1</th>\n",
       "      <th>log_Oi1_origpop</th>\n",
       "      <th>log_Dj2_destsal</th>\n",
       "      <th>log_Dist</th>\n",
       "      <th>unconstrainedEst2</th>\n",
       "      <th>fitted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>E09000001</td>\n",
       "      <td>E09000002</td>\n",
       "      <td>6</td>\n",
       "      <td>City of London</td>\n",
       "      <td>00AA</td>\n",
       "      <td>Barking and Dagenham</td>\n",
       "      <td>00AB</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>56000</td>\n",
       "      <td>16200</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15995.228542</td>\n",
       "      <td>121</td>\n",
       "      <td>9.392662</td>\n",
       "      <td>9.692767</td>\n",
       "      <td>9.680046</td>\n",
       "      <td>20</td>\n",
       "      <td>20.489486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>E09000001</td>\n",
       "      <td>E09000003</td>\n",
       "      <td>14</td>\n",
       "      <td>City of London</td>\n",
       "      <td>00AA</td>\n",
       "      <td>Barnet</td>\n",
       "      <td>00AC</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>...</td>\n",
       "      <td>159000</td>\n",
       "      <td>18700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13935.670950</td>\n",
       "      <td>184</td>\n",
       "      <td>9.392662</td>\n",
       "      <td>9.836279</td>\n",
       "      <td>9.542207</td>\n",
       "      <td>32</td>\n",
       "      <td>31.512083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>E09000001</td>\n",
       "      <td>E09000004</td>\n",
       "      <td>0</td>\n",
       "      <td>City of London</td>\n",
       "      <td>00AA</td>\n",
       "      <td>Bexley</td>\n",
       "      <td>00AD</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>112000</td>\n",
       "      <td>18300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>17359.908682</td>\n",
       "      <td>116</td>\n",
       "      <td>9.392662</td>\n",
       "      <td>9.814656</td>\n",
       "      <td>9.761919</td>\n",
       "      <td>22</td>\n",
       "      <td>22.318539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>E09000001</td>\n",
       "      <td>E09000005</td>\n",
       "      <td>16</td>\n",
       "      <td>City of London</td>\n",
       "      <td>00AA</td>\n",
       "      <td>Brent</td>\n",
       "      <td>00AE</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>127000</td>\n",
       "      <td>16500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13119.797208</td>\n",
       "      <td>183</td>\n",
       "      <td>9.392662</td>\n",
       "      <td>9.711116</td>\n",
       "      <td>9.481878</td>\n",
       "      <td>28</td>\n",
       "      <td>27.914377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>E09000001</td>\n",
       "      <td>E09000006</td>\n",
       "      <td>0</td>\n",
       "      <td>City of London</td>\n",
       "      <td>00AA</td>\n",
       "      <td>Bromley</td>\n",
       "      <td>00AF</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>164000</td>\n",
       "      <td>19100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>18740.822949</td>\n",
       "      <td>104</td>\n",
       "      <td>9.392662</td>\n",
       "      <td>9.857444</td>\n",
       "      <td>9.838459</td>\n",
       "      <td>22</td>\n",
       "      <td>21.501729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>E09000001</td>\n",
       "      <td>E09000007</td>\n",
       "      <td>335</td>\n",
       "      <td>City of London</td>\n",
       "      <td>00AA</td>\n",
       "      <td>Camden</td>\n",
       "      <td>00AG</td>\n",
       "      <td>335</td>\n",
       "      <td>0</td>\n",
       "      <td>104</td>\n",
       "      <td>...</td>\n",
       "      <td>101000</td>\n",
       "      <td>19800</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5742.818336</td>\n",
       "      <td>1148</td>\n",
       "      <td>9.392662</td>\n",
       "      <td>9.893437</td>\n",
       "      <td>8.655705</td>\n",
       "      <td>121</td>\n",
       "      <td>120.620310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>E09000002</td>\n",
       "      <td>E09000001</td>\n",
       "      <td>3641</td>\n",
       "      <td>Barking and Dagenham</td>\n",
       "      <td>00AB</td>\n",
       "      <td>City of London</td>\n",
       "      <td>00AA</td>\n",
       "      <td>3641</td>\n",
       "      <td>0</td>\n",
       "      <td>1444</td>\n",
       "      <td>...</td>\n",
       "      <td>12000</td>\n",
       "      <td>38300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15995.228542</td>\n",
       "      <td>1335</td>\n",
       "      <td>10.933107</td>\n",
       "      <td>10.553205</td>\n",
       "      <td>9.680046</td>\n",
       "      <td>1264</td>\n",
       "      <td>1263.781820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>E09000002</td>\n",
       "      <td>E09000003</td>\n",
       "      <td>194</td>\n",
       "      <td>Barking and Dagenham</td>\n",
       "      <td>00AB</td>\n",
       "      <td>Barnet</td>\n",
       "      <td>00AC</td>\n",
       "      <td>194</td>\n",
       "      <td>0</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>159000</td>\n",
       "      <td>18700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>25100.413807</td>\n",
       "      <td>265</td>\n",
       "      <td>10.933107</td>\n",
       "      <td>9.836279</td>\n",
       "      <td>10.130640</td>\n",
       "      <td>206</td>\n",
       "      <td>205.725662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>E09000002</td>\n",
       "      <td>E09000004</td>\n",
       "      <td>96</td>\n",
       "      <td>Barking and Dagenham</td>\n",
       "      <td>00AB</td>\n",
       "      <td>Bexley</td>\n",
       "      <td>00AD</td>\n",
       "      <td>96</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>112000</td>\n",
       "      <td>18300</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9264.887002</td>\n",
       "      <td>1902</td>\n",
       "      <td>10.933107</td>\n",
       "      <td>9.814656</td>\n",
       "      <td>9.133987</td>\n",
       "      <td>808</td>\n",
       "      <td>807.656203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>E09000002</td>\n",
       "      <td>E09000005</td>\n",
       "      <td>178</td>\n",
       "      <td>Barking and Dagenham</td>\n",
       "      <td>00AB</td>\n",
       "      <td>Brent</td>\n",
       "      <td>00AE</td>\n",
       "      <td>178</td>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>...</td>\n",
       "      <td>127000</td>\n",
       "      <td>16500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>27856.177167</td>\n",
       "      <td>190</td>\n",
       "      <td>10.933107</td>\n",
       "      <td>9.711116</td>\n",
       "      <td>10.234810</td>\n",
       "      <td>145</td>\n",
       "      <td>144.562407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   OrigCodeNew DestCodeNew  TotalNoIntra                  Orig OrigCode  \\\n",
       "1    E09000001   E09000002             6        City of London     00AA   \n",
       "2    E09000001   E09000003            14        City of London     00AA   \n",
       "3    E09000001   E09000004             0        City of London     00AA   \n",
       "4    E09000001   E09000005            16        City of London     00AA   \n",
       "5    E09000001   E09000006             0        City of London     00AA   \n",
       "6    E09000001   E09000007           335        City of London     00AA   \n",
       "33   E09000002   E09000001          3641  Barking and Dagenham     00AB   \n",
       "35   E09000002   E09000003           194  Barking and Dagenham     00AB   \n",
       "36   E09000002   E09000004            96  Barking and Dagenham     00AB   \n",
       "37   E09000002   E09000005           178  Barking and Dagenham     00AB   \n",
       "\n",
       "                    Dest DestCode  Total  WorksFromHome  Underground  ...  \\\n",
       "1   Barking and Dagenham     00AB      6              0            3  ...   \n",
       "2                 Barnet     00AC     14              0           11  ...   \n",
       "3                 Bexley     00AD      0              0            0  ...   \n",
       "4                  Brent     00AE     16              0           10  ...   \n",
       "5                Bromley     00AF      0              0            0  ...   \n",
       "6                 Camden     00AG    335              0          104  ...   \n",
       "33        City of London     00AA   3641              0         1444  ...   \n",
       "35                Barnet     00AC    194              0           29  ...   \n",
       "36                Bexley     00AD     96              0            6  ...   \n",
       "37                 Brent     00AE    178              0           47  ...   \n",
       "\n",
       "    Dj1_destpop  Dj2_destsal  offset          Dist  unconstrainedEst1  \\\n",
       "1         56000        16200     0.0  15995.228542                121   \n",
       "2        159000        18700     0.0  13935.670950                184   \n",
       "3        112000        18300     0.0  17359.908682                116   \n",
       "4        127000        16500     0.0  13119.797208                183   \n",
       "5        164000        19100     0.0  18740.822949                104   \n",
       "6        101000        19800     0.0   5742.818336               1148   \n",
       "33        12000        38300     0.0  15995.228542               1335   \n",
       "35       159000        18700     0.0  25100.413807                265   \n",
       "36       112000        18300     0.0   9264.887002               1902   \n",
       "37       127000        16500     0.0  27856.177167                190   \n",
       "\n",
       "    log_Oi1_origpop  log_Dj2_destsal   log_Dist  unconstrainedEst2  \\\n",
       "1          9.392662         9.692767   9.680046                 20   \n",
       "2          9.392662         9.836279   9.542207                 32   \n",
       "3          9.392662         9.814656   9.761919                 22   \n",
       "4          9.392662         9.711116   9.481878                 28   \n",
       "5          9.392662         9.857444   9.838459                 22   \n",
       "6          9.392662         9.893437   8.655705                121   \n",
       "33        10.933107        10.553205   9.680046               1264   \n",
       "35        10.933107         9.836279  10.130640                206   \n",
       "36        10.933107         9.814656   9.133987                808   \n",
       "37        10.933107         9.711116  10.234810                145   \n",
       "\n",
       "         fitted  \n",
       "1     20.489486  \n",
       "2     31.512083  \n",
       "3     22.318539  \n",
       "4     27.914377  \n",
       "5     21.501729  \n",
       "6    120.620310  \n",
       "33  1263.781820  \n",
       "35   205.725662  \n",
       "36   807.656203  \n",
       "37   144.562407  \n",
       "\n",
       "[10 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cdatasub.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Dest</th>\n",
       "      <th>Barking and Dagenham</th>\n",
       "      <th>Barnet</th>\n",
       "      <th>Bexley</th>\n",
       "      <th>Brent</th>\n",
       "      <th>Bromley</th>\n",
       "      <th>Camden</th>\n",
       "      <th>City of London</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orig</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Barking and Dagenham</th>\n",
       "      <td>NaN</td>\n",
       "      <td>194.0</td>\n",
       "      <td>96.0</td>\n",
       "      <td>178.0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>3641.0</td>\n",
       "      <td>5675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Barnet</th>\n",
       "      <td>96.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>34.0</td>\n",
       "      <td>5467.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>12080.0</td>\n",
       "      <td>7709.0</td>\n",
       "      <td>25462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bexley</th>\n",
       "      <td>362.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>144.0</td>\n",
       "      <td>4998.0</td>\n",
       "      <td>2470.0</td>\n",
       "      <td>6580.0</td>\n",
       "      <td>14686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brent</th>\n",
       "      <td>40.0</td>\n",
       "      <td>6124.0</td>\n",
       "      <td>28.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>66.0</td>\n",
       "      <td>8105.0</td>\n",
       "      <td>4145.0</td>\n",
       "      <td>18508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bromley</th>\n",
       "      <td>134.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>3199.0</td>\n",
       "      <td>201.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3780.0</td>\n",
       "      <td>9855.0</td>\n",
       "      <td>17331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Camden</th>\n",
       "      <td>36.0</td>\n",
       "      <td>1496.0</td>\n",
       "      <td>32.0</td>\n",
       "      <td>1350.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8795.0</td>\n",
       "      <td>11769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>City of London</th>\n",
       "      <td>6.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>335.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>674.0</td>\n",
       "      <td>8122.0</td>\n",
       "      <td>3389.0</td>\n",
       "      <td>7356.0</td>\n",
       "      <td>5266.0</td>\n",
       "      <td>28270.0</td>\n",
       "      <td>40725.0</td>\n",
       "      <td>93802</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Dest                  Barking and Dagenham  Barnet  Bexley   Brent  Bromley  \\\n",
       "Orig                                                                          \n",
       "Barking and Dagenham                   NaN   194.0    96.0   178.0     66.0   \n",
       "Barnet                                96.0     NaN    34.0  5467.0     76.0   \n",
       "Bexley                               362.0   132.0     NaN   144.0   4998.0   \n",
       "Brent                                 40.0  6124.0    28.0     NaN     66.0   \n",
       "Bromley                              134.0   162.0  3199.0   201.0      NaN   \n",
       "Camden                                36.0  1496.0    32.0  1350.0     60.0   \n",
       "City of London                         6.0    14.0     0.0    16.0      0.0   \n",
       "All                                  674.0  8122.0  3389.0  7356.0   5266.0   \n",
       "\n",
       "Dest                   Camden  City of London    All  \n",
       "Orig                                                  \n",
       "Barking and Dagenham   1500.0          3641.0   5675  \n",
       "Barnet                12080.0          7709.0  25462  \n",
       "Bexley                 2470.0          6580.0  14686  \n",
       "Brent                  8105.0          4145.0  18508  \n",
       "Bromley                3780.0          9855.0  17331  \n",
       "Camden                    NaN          8795.0  11769  \n",
       "City of London          335.0             NaN    371  \n",
       "All                   28270.0         40725.0  93802  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show the actual flows between boroughs\n",
    "cdatasubmat = pd.pivot_table(cdatasub, values =\"TotalNoIntra\", index=\"Orig\", columns = \"Dest\",\n",
    "                            aggfunc=np.sum, margins=True)\n",
    "#show the data\n",
    "cdatasubmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Doubly Constrained Model\n",
    "\n",
    "For the doubly constrained model:\n",
    "\n",
    "- We might be interested in understanding the changing patterns of commuting or migration over time. Data from the Census allows us to know an accurate snap-shot of migrating and commuting patterns every 10 years. In these full data matrices, we know both the numbers of commuters/migrants leaving origins and arriving at destinations as well as the interactions between them. If we constrain our model estimates to this known information at origin and destination, we can examine various things, including:\n",
    "    - The ways that the patterns of commuting/migration differ from the model predictions - where we might get more migrant/commuter flows than we would expect\n",
    "    - How the model parameters vary over time - for example how does distance / cost of travel affect flows over time? Are people prepared to travel further or less far than before?\n",
    "\n",
    "\n",
    "Which we can now introduce.\n",
    "\n",
    "Let us begin with the formula:\n",
    "\n",
    "\\begin{equation} \\tag{9}\n",
    "T_{ij} = A_i B_j O_i D_j d_{ij}^{-\\beta}\n",
    "\\end{equation}\n",
    "\n",
    "Where\n",
    "\n",
    "\\begin{equation} \\tag{10}\n",
    "O_i = \\sum_j T_{ij}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation} \\tag{11}\n",
    "D_j = \\sum_i T_{ij} \n",
    "\\end{equation}\n",
    "\n",
    "and\n",
    "\n",
    "\\begin{equation} \\tag{12}\n",
    "A_i = \\frac{1}{\\sum_j B_j D_j d_{ij}^{-\\beta}}\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation} \\tag{13}\n",
    "B_j = \\frac{1}{\\sum_i A_i O_i d_{ij}^{-\\beta}}\n",
    "\\end{equation}\n",
    "\n",
    "Now, the astute will have noticed that the calculation of $A_i$ relies on knowing $B_j$ and the calculation of $B_j$ relies on knowing $A_i$. A conundrum!! If I don’t know $A_i$ how can I calcuate $B_j$ and then in turn $A_i$ and then $B_j$ ad infinitum???!!\n",
    "\n",
    "Well, I wrestled with that for a while until I came across [this paper by Martyn Senior](http://journals.sagepub.com/doi/abs/10.1177/030913257900300218) where he sketches out a very useful algorithm for iteratively arriving at values for $A_i$ and $B_j$ by setting each to equal to 1 initially and then continuing to calculate each in turn until the difference between each value is small enough not to matter.\n",
    "\n",
    "We will return to this later, but for now, we will once again used the awesome power of Python to deal with all this difficulty for us!\n",
    "\n",
    "We can run the doubly constrained model in exactly the same way as we ran the singly constrained models:\n",
    "\n",
    "\\begin{equation} \\tag{14}\n",
    "\\lambda_{ij} = \\exp (\\alpha_i + \\gamma_j -\\beta \\ln d_{ij})\n",
    "\\end{equation}\n",
    "\n",
    "now in python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the formula (the \"-1\" indicates no intercept in the regression model).\n",
    "dbl_form = 'Total ~ Dest + Orig + log_Dist-1'\n",
    "#run a doubly constrained sim\n",
    "doubSim = smf.glm(formula = dbl_form, data=cdatasub, family=sm.families.Poisson()).fit()\n",
    "#let's have a look at it's summary\n",
    "print(doubSim.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the various flows and goodness-of-fit statistics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the estimates\n",
    "cdatasub[\"doubsimfitted\"] = np.round(doubSim.mu)\n",
    "#here's the matrix\n",
    "cdatasubmat7 = cdatasub.pivot_table(values =\"doubsimfitted\", index=\"Orig\", columns = \"Dest\",\n",
    "                                    aggfunc=np.sum, margins=True)\n",
    "cdatasubmat7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compared to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdatasubmat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and we can test the goodness-of-fit ine xactly the same was as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CalcRSqaured(cdatasub[\"Total\"],cdatasub[\"doubsimfitted\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CalcRMSE(cdatasub[\"Total\"],cdatasub[\"doubsimfitted\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the goodness of fit has shot up and we can clearly see the origin and destination constraints working, and for most sets of flows, the mdoel is now producing some good estimates. However, there are still some errors in the flows, particularly for estimates between Barking and Dagenham and Bexley or Barnet and Camden.\n",
    "\n",
    "Is there anything more we can do? Yes, of course there is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Tweaking our model\n",
    "\n",
    "### 4.1.1 Distance Decay\n",
    "\n",
    "Now, all of the way through these practicals, we have assumed that the distance decay parameter follows a negative power law. Well, it doesn't need to.\n",
    "\n",
    "In [Wilson's original paper](http://journals.sagepub.com/doi/abs/10.1068/a030001) he generalised the distance decay parameter to:\n",
    "\n",
    "\\begin{equation} \\tag{15}\n",
    "f(d_{ij})\n",
    "\\end{equation}\n",
    "\n",
    "Where $f$ represents some function of distance describing the rate at which the flow interactions change as distance increase. Lots of people have written about this, including [Talyor](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1538-4632.1971.tb00364.x) and more recently Robin Lovelace in a transport context, [here](https://www.slideshare.net/ITSLeeds/estimating-distance-decay-for-the-national-propensity-to-cycle-tool).\n",
    "\n",
    "For the inverse power law that we have been using one pussible function of distance, the other common one that is used is the negative exponential function:\n",
    "\n",
    "\\begin{equation} \\tag{16}\n",
    "\\exp (-\\beta d_{ij})\n",
    "\\end{equation}\n",
    "\n",
    "We can get a feel for how different distance decay parameters work by plotting some sample data (try different parameters):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's consider some model tweaks.\n",
    "# Starting with distance decay.\n",
    "# Let's graph the distance decay parameters to get a feel for how they work.\n",
    "xs = np.arange(1.0,20.0,0.25)\n",
    "# inverse square power\n",
    "y_inv_power = np.power(xs,-2)\n",
    "# negative exponential, beta = 0.3\n",
    "y_neg_exp_point3 = np.exp(-0.3*xs)\n",
    "\n",
    "# Now a plot.\n",
    "f, ax = plt.subplots(figsize=(8,8))\n",
    "ax.plot(xs,y_inv_power, label = \"Inverse Power\", color = \"red\")\n",
    "ax.plot(xs,y_neg_exp_point3, label = 'Negative Exponential', color = \"lightblue\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With these parameters, the inverse power function has a far more rapid distance decay effect than the negative exponential function. In real life, what this means is that if the observed interactions drop off very rapidly with distance, then they might be more likely to follow an inverse power law. This might be the case when looking at trips to the local convenience store by walking, for example. On the other hand, if the effect of distance is less severe (for example migration across the country for a new job) then the negative exponential funtion might be more appropriate.\n",
    "\n",
    "There is no hard and fast rule as to which function to pick, it will just come down to which fits the data better…\n",
    "\n",
    "As [Taylor Oshan points out in his excellent Primer](http://openjournals.wu.ac.at/region/paper_175/175.html) what this means in our Poisson regression model is that we simply substitute $-\\beta \\ln d_{ij}$ for $-\\beta d_{ij}$ in our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a doubly constrained SIM with a negative exponential cost function.\n",
    "doubsim_form = \"Total ~ Orig + Dest + Dist -1\"\n",
    "doubsim1 = smf.glm(formula=doubsim_form, data = cdatasub, family = sm.families.Poisson()).fit()\n",
    "print(doubsim1.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cdatasub[\"doubsimfitted1\"] = np.round(doubsim1.mu,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CalcRSqaured(cdatasub[\"Total\"],cdatasub[\"doubsimfitted1\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "CalcRMSE(cdatasub[\"Total\"],cdatasub[\"doubsimfitted1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that using a negative exponential in our model actually improves the fit and reduces the RMSE score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Bunging some more variables in\n",
    "\n",
    "Yes, the nice thing about doing all of this in a regression modelling framework is that we can just keep adding predictor variables into the mix and seeing whether they have an effect.\n",
    "\n",
    "You can't add origin or destination specific predictors into a doubly constrained model like this (To see why see the paper by [Flowerdew and Lovett 1988](https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1538-4632.1988.tb00184.x)) however, you could add some interaction predictors. For example, instead of modelling total flows, we could try and model motorbike commuters using information on car and underground commuters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KitchenSinkForm = \"Motobike ~ Orig + Dest + Dist +CarDrive +Underground -1\"\n",
    "\n",
    "KitchenSinkSim = smf.glm(formula=KitchenSinkForm, data = cdatasub, family = sm.families.Poisson()).fit()\n",
    "print(KitchenSinkSim.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So now it gets guite interesting. Some of the dummy/constraint origins become statistically insignificant when car and tube commuters are added into the mix.\n",
    "\n",
    "How can we interpret this?\n",
    "\n",
    "Well, the kind of things that might influence commuting by motorbike patterns (lack of access to public transport, distance from workplace etc.) that also influence travel to work by car, aren’t applicable to centrally-located Camden with lots of public transport links. Camden is just a proxy for these factors (being centrally-located Camden with lots of public transport links), but of-course doesn’t capture the subtle variation in access to public transport and distance that car travel does. Camden’s influence is [confounded](https://en.wikipedia.org/wiki/Confounding) by these better explanatory variables and becomes insignificant.\n",
    "\n",
    "The parameter values give an indication of exactly how much of a change in commuting flows by motorcycle you might expect either for beginning or ended in a borough or for a one person change in people travelling by Car or Tube.\n",
    "\n",
    "If you would like some more useful ifnromation on how to interpret the parameters (logged or otherwise) that emerge from a Poisson Regression model, again, [Taylot Oshan's primer](http://openjournals.wu.ac.at/region/paper_175/175.html) is an excellent place to turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Poisson Regression back to Entropy\n",
    "\n",
    "As with the earlier models, I have shown you how you can plug the parameter estimates back into Wilson’s entropy maximising multiplicative models in order to generate estimates and tweak things still further.\n",
    "\n",
    "If you remember from Equations 11 and 12 above, the key to the doubly constrained models is the $A_i$ and $B_j$ balancing factors and as they rely on each other, they need to be calculated iteratively. We can do this using [Senior’s algorthim](http://journals.sagepub.com/doi/abs/10.1177/030913257900300218) also mentioned earlier.\n",
    "\n",
    "Here is the code as provided by [Dan Lewis](https://github.com/danlewis85/UCL_CASA_Urban_Simulation/blob/master/Constrained%20SIM.ipynb) who in a departure from Dennet rewrites the algorithm as a function, which can then be called subject to the required parameters. In order for it to work it requires:\n",
    "\n",
    "- pd - a pandas dataframe of origin-destination pairwise flows and associated data.\n",
    "- orig_field - the name of the dataframe field in pd that uniquely labels origin zones.\n",
    "- dest_field - the name of the dataframe field in pd that uniquely labels destination zones.\n",
    "- Oi_field - the name of the dataframe field that stores total flows from a given origin $i$\n",
    "- Dj_field - the name of the dataframe field that stores total flows to a given destination $j$\n",
    "- cij_field - the name of the dataframe field that stores the pairwise cost (e.g. distance) between $i$ and $j$\n",
    "- beta - a constant for the beta parameter you wish to use in the model\n",
    "- cost_function - a string representing the cost function, either 'power' or 'exponential'\n",
    "- Ainame - What you want to call the new field in pd that will hold $A_{i}$ values, defaults to \"Ai_new\"\n",
    "- Bjname - What you want to call the new field in pd that will hold $B_{j}$ values, defaults to \"Bj_new\"\n",
    "- converge - A threshold value at which a model can be said to have converged, the default of 0.001 seems to work fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#create some Oi and Dj columns in the dataframe and store row and column totals in them:\n",
    "#to create O_i, take cdatasub ...then... group by origcodenew ...then... summarise by calculating the sum of Total\n",
    "O_i = pd.DataFrame(cdatasub.groupby([\"OrigCodeNew\"])[\"Total\"].agg(np.sum))\n",
    "O_i.rename(columns={\"Total\":\"O_i\"}, inplace = True)\n",
    "cdatasub = cdatasub.merge(O_i, on = \"OrigCodeNew\", how = \"left\" )\n",
    "\n",
    "D_j = pd.DataFrame(cdatasub.groupby([\"DestCodeNew\"])[\"Total\"].agg(np.sum))\n",
    "D_j.rename(columns={\"Total\":\"D_j\"}, inplace = True)\n",
    "cdatasub = cdatasub.merge(D_j, on = \"DestCodeNew\", how = \"left\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Here is the entropy maximising approach for a known beta.\n",
    "# Plug in the required values in this function to solve.\n",
    "\n",
    "def balance_doubly_constrained(pd, orig_field, dest_field, Oi_field, Dj_field, cij_field, beta, \n",
    "                               cost_function, Ai_name = \"Ai_new\", Bj_name = \"Bj_new\", converge=0.001):\n",
    "    # Define some variables\n",
    "    Oi = pd[[orig_field, Oi_field]]\n",
    "    Dj = pd[[dest_field,Dj_field]]    \n",
    "    if cost_function.lower() in ['power','pow']:\n",
    "        beta_cij = np.exp(beta * np.log(pd[cij_field]))\n",
    "    elif cost_function.lower() in ['exponential','exp']:\n",
    "        beta_cij = np.exp(beta * pd[cij_field])\n",
    "    else:\n",
    "        return \"Cost function not specified properly, use 'exp' or 'pow'\"\n",
    "    \n",
    "    # Create some helper variables\n",
    "    cnvg = 1\n",
    "    iteration = 0\n",
    "    # Now iteratively rebalance the Ai and Bj terms until convergence\n",
    "    while cnvg > converge:\n",
    "        if iteration == 0:\n",
    "            # This first condition sets starting values for Ai and Bj\n",
    "            # NB sets starting value of Ai assuming Bj is a vector of 1s.\n",
    "            # We've already established beta_cij with the appropriate cost function, so...\n",
    "            Oi = Oi.assign(Ai = Dj[Dj_field] * beta_cij)\n",
    "            # Aggregate Ai and take inverse\n",
    "            Ai = 1.0/Oi.groupby(orig_field)['Ai'].sum().to_frame()\n",
    "            # Merge new Ais \n",
    "            Oi = Oi.merge(Ai,left_on = orig_field, right_index = True, suffixes = ('','_old'))\n",
    "            # Drop the temporary Ai field we created, leaving Ai_old\n",
    "            Oi.drop('Ai', axis=1, inplace=True)\n",
    "            \n",
    "            # Now set up Bjs using starting values of Ai\n",
    "            Dj = Dj.assign(Bj = Oi['Ai_old'] * Oi[Oi_field] * beta_cij)\n",
    "            # Aggregate Bj and take inverse\n",
    "            Bj = 1.0/Dj.groupby(dest_field)['Bj'].sum().to_frame()\n",
    "            # Merge new Bjs\n",
    "            Dj = Dj.merge(Bj,left_on = dest_field, right_index = True, suffixes = ('','_old'))\n",
    "            # Drop the temporary Bj field we created, leaving Bj_old\n",
    "            Dj.drop('Bj', axis=1, inplace=True)\n",
    "            \n",
    "            # Increment loop\n",
    "            iteration += 1\n",
    "        else:\n",
    "            # This bit is the iterated bit of the loop which refines the values of Ai and Bj\n",
    "            # First Ai\n",
    "            Oi['Ai'] = Dj['Bj_old'] * Dj[Dj_field] * beta_cij\n",
    "            # Aggregate Ai and take inverse\n",
    "            Ai = 1.0/Oi.groupby(orig_field)['Ai'].sum().to_frame()\n",
    "            # Drop temporary Ai\n",
    "            Oi.drop('Ai', axis=1, inplace=True)\n",
    "            # Merge new Ais \n",
    "            Oi = Oi.merge(Ai,left_on = orig_field, right_index = True)\n",
    "            # Calculate the difference between old and new Ais\n",
    "            Oi['diff'] = np.absolute((Oi['Ai_old'] - Oi['Ai'])/Oi['Ai_old'])\n",
    "            # Set new Ais to Ai_old\n",
    "            Oi['Ai_old'] = Oi['Ai']\n",
    "            # Drop the temporary Ai field we created, leaving Ai_old\n",
    "            Oi.drop('Ai', axis=1, inplace=True)\n",
    "            \n",
    "            # Then Bj\n",
    "            Dj['Bj'] = Oi['Ai_old'] * Oi[Oi_field] * beta_cij\n",
    "            # Aggregate Bj and take inverse\n",
    "            Bj = 1.0/Dj.groupby(dest_field)['Bj'].sum().to_frame()\n",
    "            # Drop temporary Bj\n",
    "            Dj.drop('Bj', axis=1, inplace=True)\n",
    "            # Merge new Bjs\n",
    "            Dj = Dj.merge(Bj,left_on = dest_field, right_index = True)\n",
    "            # Calculate the difference between old and new Bjs\n",
    "            Dj['diff'] = np.absolute((Dj['Bj_old'] - Dj['Bj'])/Dj['Bj_old'])\n",
    "            # Set new Bjs to Bj_old\n",
    "            Dj['Bj_old'] = Dj['Bj']\n",
    "            # Drop the temporary Bj field we created, leaving Bj_old\n",
    "            Dj.drop('Bj', axis=1, inplace=True)\n",
    "            \n",
    "            # Assign higher sum difference from Ai or Bj to cnvg\n",
    "            cnvg = np.maximum(Oi['diff'].sum(),Dj['diff'].sum())\n",
    "            \n",
    "            # Print and increment loop\n",
    "            print(\"Iteration:\", iteration)\n",
    "            iteration += 1\n",
    "\n",
    "    # When the while loop finishes add the computed Ai_old and Bj_old to the dataframe and return\n",
    "    pd[Ai_name] = Oi['Ai_old']\n",
    "    pd[Bj_name] = Dj['Bj_old']\n",
    "    return pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using the function above we can calculate $A_{i}$ and $B_{j}$ for the previous Poisson model by plugging in the estimate of beta that we generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use the beta we got from the inverse power model\n",
    "beta = -doubSim.params[-1]\n",
    "# Get the balancing factors.\n",
    "cdatasub = balance_doubly_constrained(cdatasub,'OrigCodeNew','DestCodeNew','O_i','D_j','Dist',-beta,'power')\n",
    "\n",
    "# Now predict the model again using the new Ai and Dj fields.\n",
    "cdatasub['SIM_est_pow'] = np.round(cdatasub['O_i'] * cdatasub['Ai_new'] * cdatasub['D_j'] * cdatasub['Bj_new'] * \n",
    "                                   np.exp(np.log(cdatasub['Dist'])*-beta))\n",
    "# Check out the matrix\n",
    "pd.pivot_table(cdatasub,values='SIM_est_pow',index ='Orig',columns='Dest',fill_value=0,aggfunc=sum,margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the beta we got from the negative exponential model\n",
    "beta = -doubsim1.params[-1]\n",
    "# Get the balancing factors. NB Setting of new field names for Ai and Bj.\n",
    "cdatasub = balance_doubly_constrained(cdatasub,'OrigCodeNew','DestCodeNew','O_i','D_j','Dist',-beta,'exponential','Ai_exp','Bj_exp')\n",
    "\n",
    "# Now predict the model again using the new Ai and Dj fields.\n",
    "cdatasub['SIM_est_exp'] = np.round(cdatasub['O_i'] * cdatasub['Ai_exp'] * cdatasub['D_j'] * cdatasub['Bj_exp'] * \n",
    "                                   np.exp(cdatasub['Dist']*-beta))\n",
    "# Check out the matrix\n",
    "pd.pivot_table(cdatasub,values='SIM_est_exp',index ='Orig',columns='Dest',fill_value=0,aggfunc=sum,margins=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Conclusions, further notes and ideas for additional activities\n",
    "Hopefully you have now seen how it is extremely straight-forward to run and calibrate Wilson’s full family of Spatial Interaction Models in Python using GLM and Poisson Regression.\n",
    "\n",
    "### 5.1 Some Further Notes\n",
    "Now might be the time to mention that despite everything I’ve shown you, there has been some discussion in the literature as to whether the Poisson Model is actually a misspecification, especially for modelling migration flows. If you have the stomach for it, [this paper by Congdon goes into a lot of detail.](http://journals.sagepub.com/doi/abs/10.1068/a251481)\n",
    "\n",
    "The issue is a thing called ‘overdispersion’ which, translated, essentially relates to the model not being able to capture all of the things that could be explaining the flows in the independent variables that are supplied to the model. The details are tedious and only really intelligible to those with a statistics background. If you want a starter, [try here](https://en.wikipedia.org/wiki/Overdispersion), but in practical terms, we can get around this problem by fitting a very similar sort of regression model called the negative binomial regression model.\n",
    "\n",
    "If you wish, you can read up and experiment with this model - you can fit it in exactly the same way as the poisson glm model but using: family = sm.families.NegativeBinomial(alpha) when you call the statsmodel glm function. The negative binomial model has an extra parameter - alpha - in the model for overdispersion with a default of 1. If you do try this, you will almost certainly discover that your results barely change - but hell, you might keep a pedantic reviewer at bay if you submit this to a journal (not that I’m speaking from experience or anything).\n",
    "\n",
    "### And some more comments\n",
    "\n",
    "Another thing to note is that the example we used here had quite neat data. You will almost certainly run into problems if you have sparse data or predictors with 0s in them. If this happens, then you might need to either drop some rows in your data (if populated with 0s) or substitute 0s for very small numbers, much less than 1, but greater than 0 (this is because you can’t take the log of 0). [Taylor Oshan's SpInt](http://openjournals.wu.ac.at/region/paper_175/175.html) implementation in Python uses a special Poisson regression approach that better handles sparse data structures.\n",
    "\n",
    "And another thing to note is that our flow data and our predictors were all in and around the same order or magnitude. If you suddenly get data that (such as population masses at origins and destinations) that are an order of magnitude different (i.e. populations about ten times larger in different locations) then the model estimates might be biased. Fortunately, there are packages available to help us with these problems as well.\n",
    "\n",
    "### 5.2 Further Activities\n",
    "1. Testing these models out on the whole of London and for different years\n",
    "    - You’ve been playing around with just a small 7 borough sample, why not try the full London system. +You can also try and download some similar data from the 2011 Census from [Wicid](http://wicid.ukdataservice.ac.uk/) - see if using Oi and Dj totals and the parameters you calibrated on the 2001 data, whether you can get reasonable estimates of the 2011 flows. +How have the model parameters changed between 2001 and 2011 - what does this mean\n",
    "2. Visualising your flow estimates\n",
    "    - try using the methods  last practical to visualise some of your flow estimates or flow residuals…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Challenge to complete\n",
    "\n",
    "The challenge to complete is to run every possible model (unconstrained, production constrained, attraction constrained and doubly constrained) with each distance decay function (power, exponential) on the total dataset and to extract each models paramaters, along with each models performance metrics ($R2$, $RMSE$). For this, you will need to go back to the original workshop to be able to extract the full `cdata` dataset which will be used here (push the data to a csv and then read it in here).\n",
    "\n",
    "At this point you should be able to cobble togther rather basic code to implement each model and then extract their performance and paramaters. To this end, you will not be given any comment hints. You can do this manually or you could create a loop to iterate over each model to save on code, this is up to you. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Think about: \n",
    "- What do these paramaters mean?\n",
    "- Why does one type of distance decay fit the data better than another?\n",
    "- Would this change with scale or different data?\n",
    "- How do these results compare to the subset of data?\n",
    "- What other data could we use in these models to either improve performance or improve their usefulness?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
